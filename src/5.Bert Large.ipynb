{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Bert Large.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"53fb8faa00e3446ca122d11f1baff16e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_c51524348f7144ec8ccb8aff2d1b7d8e","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_17162273cb5840f7b4cd5d01ca9cf77a","IPY_MODEL_b045f3c381dd4b8fb28018e1a4e10c03"]}},"c51524348f7144ec8ccb8aff2d1b7d8e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"17162273cb5840f7b4cd5d01ca9cf77a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_4e36c890e8b44ef5b317ded1e7267323","_dom_classes":[],"description":"  5%","_model_name":"FloatProgressModel","bar_style":"","max":688,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":37,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_2431c9fcc00a4ad086723020f39b975a"}},"b045f3c381dd4b8fb28018e1a4e10c03":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_4b73977367a744a1ad7bd8e08fe165cc","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"â€‹","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 37/688 [00:47&lt;13:23,  1.23s/it, jaccard=0.552, loss=2.34]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_6e3292261054423dbd017337294f8652"}},"4e36c890e8b44ef5b317ded1e7267323":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"2431c9fcc00a4ad086723020f39b975a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4b73977367a744a1ad7bd8e08fe165cc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"6e3292261054423dbd017337294f8652":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"id":"4MYEP33bH2pJ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":381},"executionInfo":{"status":"ok","timestamp":1593837627400,"user_tz":300,"elapsed":3103,"user":{"displayName":"Abhishek Reddy Palle","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgAOUr87agaYwb02gOAWZsxpouYO6t3YKRAuHZHqg=s64","userId":"04844477878113654498"}},"outputId":"ff26fbca-2703-47a0-85e0-fc7ca53f95e7"},"source":["!nvidia-smi"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Sat Jul  4 04:40:25 2020       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 450.36.06    Driver Version: 418.67       CUDA Version: 10.1     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   46C    P0    29W / 250W |      0MiB / 16280MiB |      0%      Default |\n","|                               |                      |                 ERR! |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hi4iWotlgiGq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":74},"executionInfo":{"status":"ok","timestamp":1593837529982,"user_tz":300,"elapsed":1094,"user":{"displayName":"Abhishek Reddy Palle","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgAOUr87agaYwb02gOAWZsxpouYO6t3YKRAuHZHqg=s64","userId":"04844477878113654498"}},"outputId":"52c94593-ed68-48ea-db85-aca6f0866d88"},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","%cd /content/drive/My Drive/Tweet Sentiment Extraction Final"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/My Drive/Tweet Sentiment Extraction Final\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9rVu1fCJA_a_","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":383},"executionInfo":{"status":"ok","timestamp":1593837534030,"user_tz":300,"elapsed":5120,"user":{"displayName":"Abhishek Reddy Palle","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgAOUr87agaYwb02gOAWZsxpouYO6t3YKRAuHZHqg=s64","userId":"04844477878113654498"}},"outputId":"cd28b73e-714f-4a44-a0cc-fde5b82e7e5a"},"source":["!pip install transformers"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.0.1)\n","Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: tokenizers==0.8.0-rc4 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8.0rc4)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n","Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.15.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"61JjdybxSnaa","colab_type":"text"},"source":["# Utils"]},{"cell_type":"code","metadata":{"id":"GdTtQZmuSm3r","colab_type":"code","colab":{}},"source":["import os\n","import random\n","\n","import torch\n","import numpy as np\n","\n","class utils:\n","  def seed_everything(seed):\n","      random.seed(seed)\n","      os.environ['PYTHONHASHSEED'] = str(seed)\n","      torch.manual_seed(seed)\n","      torch.backends.cudnn.deterministic = True\n","      torch.backends.cudnn.benchmark = False\n","      np.random.seed(seed)\n","\n","\n","  def token_level_to_char_level(text, offsets, preds):\n","      probas_char = np.zeros(len(text))\n","      for i, offset in enumerate(offsets):\n","          if offset[0] or offset[1]:\n","              probas_char[offset[0]:offset[1]] = preds[i]\n","\n","      return probas_char\n","\n","\n","  def jaccard(str1, str2):\n","      \"\"\"Original metric implementation.\"\"\"\n","      a = set(str1.lower().split())\n","      b = set(str2.lower().split())\n","      c = a.intersection(b)\n","      return float(len(c)) / (len(a) + len(b) - len(c))\n","\n","\n","  def get_best_start_end_idx(start_logits, end_logits,\n","                            orig_start, orig_end):\n","      \"\"\"Return best start and end indices following BERT paper.\"\"\"\n","      best_logit = -np.inf\n","      best_idxs = None\n","      start_logits = start_logits[orig_start:orig_end + 1]\n","      end_logits = end_logits[orig_start:orig_end + 1]\n","      for start_idx, start_logit in enumerate(start_logits):\n","          for end_idx, end_logit in enumerate(end_logits[start_idx:]):\n","              logit_sum = start_logit + end_logit\n","              if logit_sum > best_logit:\n","                  best_logit = logit_sum\n","                  best_idxs = (orig_start + start_idx,\n","                              orig_start + start_idx + end_idx)\n","      return best_idxs\n","\n","\n","  def calculate_jaccard(original_tweet, target_string,\n","                        start_logits, end_logits,\n","                        orig_start, orig_end,\n","                        offsets, \n","                        verbose=False):\n","      \"\"\"Calculates final Jaccard score using predictions.\"\"\"\n","      start_idx, end_idx = get_best_start_end_idx(\n","          start_logits, end_logits, orig_start, orig_end)\n","\n","      filtered_output = ''\n","      for ix in range(start_idx, end_idx + 1):\n","          filtered_output += original_tweet[offsets[ix][0]:offsets[ix][1]]\n","          if (ix + 1) < len(offsets) and offsets[ix][1] < offsets[ix + 1][0]:\n","              filtered_output += ' '\n","\n","      # Return orig tweet if it has less then 2 words\n","      if len(original_tweet.split()) < 2:\n","          filtered_output = original_tweet\n","\n","      if len(filtered_output.split()) == 1:\n","          filtered_output = filtered_output.replace('!!!!', '!')\n","          filtered_output = filtered_output.replace('..', '.')\n","          filtered_output = filtered_output.replace('...', '.')\n","\n","      filtered_output = filtered_output.replace('Ã¯Ã¯', 'Ã¯')\n","      filtered_output = filtered_output.replace('Â¿Â¿', 'Â¿')\n","\n","      jac = jaccard(target_string.strip(), filtered_output.strip())\n","      return jac, filtered_output\n","\n","\n","  class AverageMeter:\n","      \"\"\"Computes and stores the average and current value.\"\"\"\n","\n","      def __init__(self):\n","          self.reset()\n","\n","      def reset(self):\n","          self.val = 0\n","          self.avg = 0\n","          self.sum = 0\n","          self.count = 0\n","\n","      def update(self, val, n=1):\n","          self.val = val\n","          self.sum += val * n\n","          self.count += n\n","          self.avg = self.sum / self.count\n","\n","  class EarlyStopping:\n","    def __init__(self, patience=7, mode=\"max\", delta=0.001):\n","      self.patience = patience\n","      self.counter = 0\n","      self.mode = mode\n","      self.best_score = None\n","      self.early_stop = False\n","      self.delta = delta\n","      if self.mode == \"min\":\n","          self.val_score = np.Inf\n","      else:\n","          self.val_score = -np.Inf\n","\n","    def __call__(self, epoch_score, model, model_path):\n","\n","      if self.mode == \"min\":\n","          score = -1.0 * epoch_score\n","      else:\n","          score = np.copy(epoch_score)\n","\n","      if self.best_score is None:\n","          self.best_score = score\n","          self.save_checkpoint(epoch_score, model, model_path)\n","      elif score < self.best_score + self.delta:\n","          self.counter += 1\n","          print('EarlyStopping counter: {} out of {}'.format(self.counter, self.patience))\n","          if self.counter >= self.patience:\n","              self.early_stop = True\n","      else:\n","          self.best_score = score\n","          self.save_checkpoint(epoch_score, model, model_path)\n","          self.counter = 0\n","\n","    def save_checkpoint(self, epoch_score, model, model_path):\n","      if epoch_score not in [-np.inf, np.inf, -np.nan, np.nan]:\n","          print('Validation score improved ({} --> {}). Saving model!'.format(self.val_score, epoch_score))\n","          torch.save(model.state_dict(), model_path)\n","      self.val_score = epoch_score"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ABIyIgoL9jyd","colab_type":"text"},"source":["# Config"]},{"cell_type":"markdown","metadata":{"id":"P33Vh87ZsiSe","colab_type":"text"},"source":["\n","```\n","MODEL_PATHS = {\n","    'bert-base-uncased': './input/bert-configs/uncased_L-12_H-768_A-12',\n","    'bert-large-uncased-whole-word-masking-finetuned-squad': './input/bertconfigs/wwm_uncased_L-24_H-1024_A-16',\n","    'albert-large-v2': './input/albert-configs/albert-large-v2',\n","    'albert-base-v2': './input/albert-configs/albert-base-v2',\n","    'distilbert': './input/albert-configs/distilbert',\n","}\n","```\n","\n","\n","\n","```\n","TRANSFORMERS = {   \n","    \"bert-base-uncased\": (BertModel, \"bert-base-uncased\", BertConfig),\n","    \"bert-large-uncased-whole-word-masking-finetuned-squad\": (BertModel, \"bert-large-uncased-whole-word-masking-finetuned-squad\", BertConfig),\n","    'albert-base-v2': (AlbertModel, 'albert-base-v2', AlbertConfig),\n","    'albert-large-v2': (AlbertModel, 'albert-large-v2', AlbertConfig),\n","    \"distilbert\": (DistilBertModel, \"distilbert-base-uncased-distilled-squad\", DistilBertConfig),\n","}\n","\n","```\n","\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"7j70FwJDkO8K","colab_type":"code","colab":{}},"source":["import tokenizers\n","\n","class config:\n","\n","  # Paths\n","  TOKENIZER_PATH = './input/bert-configs/wwm_uncased_L-24_H-1024_A-16'\n","  PRETRAINED_MODEL_PATH = './input/bert-configs/wwm_uncased_L-24_H-1024_A-16'\n","  TRAIN_FILE = './input/tweet-sentiment-extraction/train_folds.csv' \n","  TEST_FILE = './input/tweet-sentiment-extraction/test.csv'\n","  SAMPLE_SUBMISSION_FILE = './input/tweet-sentiment-extraction/sample_submission.csv' \n","  SAVE_WEIGHTS_PATH = './output/Bert Large'\n","  FINAL_SUBMISSION_FILE = './output/Bert Large' \n","\n","  # Model config\n","  MODEL_CONFIG = './input/bert-configs/wwm_uncased_L-24_H-1024_A-16'\n","\n","  # Model params\n","  SEED = 25\n","  N_FOLDS = 5\n","  EPOCHS = 5\n","  LEARNING_RATE = 4e-5\n","  PATIENCE = None\n","  EARLY_STOPPING_DELTA = None\n","  TRAIN_BATCH_SIZE = 32\n","  VALID_BATCH_SIZE = 16\n","  MAX_LEN = 128  \n","  TOKENIZER = tokenizers.BertWordPieceTokenizer(\n","        f\"{TOKENIZER_PATH}/vocab.txt\", \n","        lowercase=True\n","    )\n","  \n","  HIDDEN_SIZE = 1024\n","  N_LAST_HIDDEN = 24\n","  HIGH_DROPOUT = 0.5\n","  SOFT_ALPHA = 0.6\n","  WARMUP_RATIO = 0.25\n","  WEIGHT_DECAY = 0.001\n","  USE_SWA = False\n","  SWA_RATIO = 0.9\n","  SWA_FREQ = 30\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KN-LC6x_9znO","colab_type":"text"},"source":["# Dataset"]},{"cell_type":"code","metadata":{"id":"hctcW8Nq9zE8","colab_type":"code","colab":{}},"source":["import numpy as np\n","import torch\n","\n","def jaccard_array(a, b):\n","    \"\"\"Calculates Jaccard on arrays.\"\"\"\n","    a = set(a)\n","    b = set(b)\n","    c = a.intersection(b)\n","    return float(len(c)) / (len(a) + len(b) - len(c))\n","\n","def process_data(tweet, selected_text, sentiment,\n","                 tokenizer, max_len):\n","    \"\"\"Preprocesses one data sample and returns a dict\n","    with targets and other useful info.\n","    \"\"\"\n","    len_sel_text = len(selected_text)\n","    # Get selected_text start and end idx\n","    idx_0 = None\n","    idx_1 = None\n","    for ind in (i for i, e in enumerate(tweet) if e == selected_text[0]):\n","      if tweet[ind: ind+len_sel_text] == selected_text:\n","        idx_0 = ind\n","        idx_1 = ind + len_sel_text - 1\n","        break\n","\n","    # Assign 1 as target for each char in sel_text\n","    char_targets = [0] * len(tweet)\n","    if idx_0 is not None and idx_1 is not None:\n","        for ct in range(idx_0, idx_1 + 1):\n","            char_targets[ct] = 1\n","\n","    # Check the example - https://github.com/huggingface/tokenizers\n","    tokenized_tweet = tokenizer.encode(tweet)\n","    # Vocab ids\n","    input_ids_original = tokenized_tweet.ids[1:-1]\n","    # Start and end char\n","    tweet_offsets = tokenized_tweet.offsets[1:-1]\n","\n","    # Get ids within tweet of words that have target char\n","    target_ids = []\n","    for i, (offset_0, offset_1) in enumerate(tweet_offsets):\n","        if sum(char_targets[offset_0:offset_1]) > 0:\n","            target_ids.append(i)\n","\n","    targets_start = target_ids[0]\n","    targets_end = target_ids[-1]\n","\n","    # Sentiment 'word' id in vocab\n","    sentiment_id = {'positive': 3893,\n","                    'negative': 4997,\n","                    'neutral': 8699}\n","\n","    # Soft Jaccard labels\n","    # ----------------------------------\n","    n = len(input_ids_original)\n","    sentence = np.arange(n)\n","    answer = sentence[targets_start:targets_end + 1]\n","\n","    start_labels = np.zeros(n)\n","    for i in range(targets_end + 1):\n","        jac = jaccard_array(answer, sentence[i:targets_end + 1])\n","        start_labels[i] = jac + jac ** 2\n","    start_labels = (1 - config.SOFT_ALPHA) * start_labels / start_labels.sum()\n","    start_labels[targets_start] += config.SOFT_ALPHA\n","\n","    end_labels = np.zeros(n)\n","    for i in range(targets_start, n):\n","        jac = jaccard_array(answer, sentence[targets_start:i + 1])\n","        end_labels[i] = jac + jac ** 2\n","    end_labels = (1 - config.SOFT_ALPHA) * end_labels / end_labels.sum()\n","    end_labels[targets_end] += config.SOFT_ALPHA\n","\n","    start_labels = [0, 0, 0] + list(start_labels) + [0]\n","    end_labels = [0, 0, 0] + list(end_labels) + [0]\n","    # ----------------------------------\n","\n","    # Input for BERT\n","    input_ids = [101] + [sentiment_id[sentiment]] + [102] + input_ids_original + [102]\n","    token_type_ids = [0, 0, 0] + [1] * (len(input_ids_original) + 1)\n","    # Mask of input without padding\n","    mask = [1] * len(token_type_ids)\n","    # Start and end char ids for each word including new tokens\n","    tweet_offsets = [(0, 0)] * 3 + tweet_offsets + [(0, 0)]\n","    # Ids within tweet of words that have target char including new tokens\n","    targets_start += 3\n","    targets_end += 3\n","\n","    # Input padding: new mask, token type ids, tweet offsets\n","    padding_len = max_len - len(input_ids)\n","    if padding_len > 0:\n","        input_ids = input_ids + ([1] * padding_len)\n","        mask = mask + ([0] * padding_len)\n","        token_type_ids = token_type_ids + ([0] * padding_len)\n","        tweet_offsets = tweet_offsets + ([(0, 0)] * padding_len)\n","        start_labels = start_labels + ([0] * padding_len)\n","        end_labels = end_labels + ([0] * padding_len)\n","    else:\n","        input_ids = input_ids[:max_len]\n","        mask = mask[:max_len]\n","        token_type_ids = token_type_ids[:max_len]\n","        tweet_offsets = tweet_offsets[:max_len]\n","        start_labels = start_labels[:max_len]\n","        end_labels = end_labels[:max_len]\n","\n","    return {'ids': input_ids,\n","            'mask': mask,\n","            'token_type_ids': token_type_ids,\n","            'start_labels': start_labels,\n","            'end_labels': end_labels,\n","            'orig_tweet': tweet,\n","            'orig_selected': selected_text,\n","            'sentiment': sentiment,\n","            'offsets': tweet_offsets}\n","\n","\n","class TweetDataset:\n","    def __init__(self, tweets, sentiments, selected_texts):\n","        self.tweets = tweets\n","        self.sentiments = sentiments\n","        self.selected_texts = selected_texts\n","        self.max_len = config.MAX_LEN\n","        self.tokenizer = config.TOKENIZER\n","\n","    def __len__(self):\n","        return len(self.tweets)\n","\n","    def __getitem__(self, item):\n","        \"\"\"Returns preprocessed data sample as dict with\n","        data converted to tensors.\n","        \"\"\"\n","        data = process_data(self.tweets[item],\n","                            self.selected_texts[item],\n","                            self.sentiments[item],\n","                            self.tokenizer,\n","                            self.max_len)\n","\n","        return {'ids': torch.tensor(data['ids'], dtype=torch.long),\n","                'mask': torch.tensor(data['mask'], dtype=torch.long),\n","                'token_type_ids': torch.tensor(data['token_type_ids'],dtype=torch.long),\n","                'start_labels': torch.tensor(data['start_labels'],dtype=torch.float),\n","                'end_labels': torch.tensor(data['end_labels'],dtype=torch.float),\n","                'orig_tweet': data['orig_tweet'],\n","                'orig_selected': data['orig_selected'],\n","                'sentiment': data['sentiment'],\n","                'offsets': torch.tensor(data['offsets'], dtype=torch.long)}\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_o6eBo0d949L","colab_type":"text"},"source":["# Models"]},{"cell_type":"code","metadata":{"id":"Cdk53GpY98aa","colab_type":"code","colab":{}},"source":["import torch\n","import transformers\n","\n","class TweetModel(transformers.BertPreTrainedModel):\n","    def __init__(self, conf):\n","        super(TweetModel, self).__init__(conf)\n","        self.roberta = transformers.BertModel.from_pretrained(\n","            config.PRETRAINED_MODEL_PATH,\n","            config=conf)\n","        self.high_dropout = torch.nn.Dropout(config.HIGH_DROPOUT)\n","        self.classifier = torch.nn.Linear(config.HIDDEN_SIZE * 2, 2)\n","\n","        torch.nn.init.normal_(self.classifier.weight, std=0.02)\n","\n","    def forward(self, ids, mask, token_type_ids):\n","        # sequence_output of N_LAST_HIDDEN + Embedding states\n","        # (N_LAST_HIDDEN + 1, batch_size, num_tokens, 768)\n","        _, _, out = self.roberta(ids, attention_mask=mask,\n","                                 token_type_ids=token_type_ids)\n","\n","        out = torch.stack(\n","            tuple(out[-i - 1] for i in range(config.N_LAST_HIDDEN)), dim=0)\n","        out_mean = torch.mean(out, dim=0)\n","        out_max, _ = torch.max(out, dim=0)\n","        out = torch.cat((out_mean, out_max), dim=-1)\n","\n","        # Multisample Dropout: https://arxiv.org/abs/1905.09788\n","        logits = torch.mean(torch.stack([\n","            self.classifier(self.high_dropout(out))\n","            for _ in range(5)\n","        ], dim=0), dim=0)\n","\n","        start_logits, end_logits = logits.split(1, dim=-1)\n","\n","        # (batch_size, num_tokens)\n","        start_logits = start_logits.squeeze(-1)\n","        end_logits = end_logits.squeeze(-1)\n","\n","        return start_logits, end_logits\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B-dr7pnc_0IW","colab_type":"text"},"source":["# Loss Function"]},{"cell_type":"code","metadata":{"id":"Lsn_Kq9d_5h8","colab_type":"code","colab":{}},"source":["# outputs must be log-probabilities and labels must be probabilities \n","def loss_fn(start_logits, end_logits, start_labels, end_labels):\n","  logsoftmax = nn.LogSoftmax(dim=1)\n","  loss_fct = nn.KLDivLoss(reduction='batchmean')\n","  start_loss = loss_fct(logsoftmax(start_logits), start_labels)\n","  end_loss = loss_fct(logsoftmax(end_logits), end_labels)\n","  total_loss = (start_loss + end_loss)\n","  return total_loss"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zSAh_wfR_7sL","colab_type":"text"},"source":["# Training Function"]},{"cell_type":"code","metadata":{"id":"VJBMHMa2_9_M","colab_type":"code","colab":{}},"source":["from tqdm.autonotebook import tqdm\n","\n","def train_fn(data_loader, model, optimizer, device, scheduler=None):\n","    \n","    # First thing we want to do is put the model in train mode\n","    model.train()\n","    \n","    # Check the AverageMeter class in utils class\n","    # Instantiate the AverageMeter class to print average loss & jaccard score\n","    losses = utils.AverageMeter()\n","    jaccards = utils.AverageMeter()\n","\n","    tk0 = tqdm(data_loader, total=len(data_loader))\n","    for bi, d in enumerate(tk0):\n","        \n","        # Load data into variables\n","        ids = d[\"ids\"]\n","        mask = d[\"mask\"]\n","        token_type_ids = d[\"token_type_ids\"]\n","        start_labels = d[\"start_labels\"]\n","        end_labels = d[\"end_labels\"]\n","        orig_tweet = d[\"orig_tweet\"]\n","        orig_selected = d[\"orig_selected\"]\n","        sentiment = d[\"sentiment\"]\n","        offsets = d[\"offsets\"]\n","\n","        # Push the variables to GPU device\n","        # https://pytorch.org/docs/stable/tensor_attributes.html\n","        ids = ids.to(device, dtype=torch.long)\n","        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n","        mask = mask.to(device, dtype=torch.long)\n","        start_labels = start_labels.to(device, dtype=torch.float)\n","        end_labels = end_labels.to(device, dtype=torch.float)\n","\n","        # Clear gradients w.r.t. parameters\n","        model.zero_grad()\n","        \n","        # Forward pass to get outputs\n","        outputs_start, outputs_end = model(\n","            ids=ids,\n","            mask=mask,\n","            token_type_ids=token_type_ids,\n","        )\n","\n","        # Calculate Loss: cross entropy loss\n","        loss = loss_fn(outputs_start, outputs_end, start_labels, end_labels)\n","        \n","        # Getting gradients w.r.t. parameters\n","        loss.backward()\n","        \n","        # Updating parameters\n","        optimizer.step()\n","        scheduler.step()\n","\n","        outputs_start = torch.softmax(outputs_start, dim=1).cpu().detach().numpy()\n","        outputs_end = torch.softmax(outputs_end, dim=1).cpu().detach().numpy()\n","        \n","        jaccard_scores = []\n","        for px, tweet in enumerate(orig_tweet):\n","            selected_tweet = orig_selected[px]\n","            tweet_sentiment = sentiment[px]\n","            jaccard_score, _ = calculate_jaccard_score(\n","                original_tweet=tweet,\n","                target_string=selected_tweet,\n","                sentiment_val=tweet_sentiment,\n","                idx_start=np.argmax(outputs_start[px, :]),\n","                idx_end=np.argmax(outputs_end[px, :]),\n","                offsets=offsets[px]\n","            )\n","            jaccard_scores.append(jaccard_score)\n","\n","        # Update and print\n","        losses.update(loss.item(), ids.size(0))\n","        jaccards.update(np.mean(jaccard_scores), ids.size(0))\n","        tk0.set_postfix(loss=losses.avg, jaccard=jaccards.avg)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dtWWrFbOABfN","colab_type":"text"},"source":["# Evaluation Functions"]},{"cell_type":"code","metadata":{"id":"b7FWDGlBADKU","colab_type":"code","colab":{}},"source":["def calculate_jaccard_score(\n","    original_tweet, \n","    target_string, \n","    sentiment_val, \n","    idx_start, \n","    idx_end, \n","    offsets,\n","    verbose=False):\n","    \n","    if idx_end < idx_start:\n","        idx_end = idx_start\n","    \n","    filtered_output  = \"\"\n","    for ix in range(idx_start, idx_end + 1):\n","        filtered_output += original_tweet[offsets[ix][0]: offsets[ix][1]]\n","        if (ix+1) < len(offsets) and offsets[ix][1] < offsets[ix+1][0]:\n","            filtered_output += \" \"\n","\n","    if sentiment_val == \"neutral\" or len(original_tweet.split()) < 2:\n","        filtered_output = original_tweet\n","\n","    jac = utils.jaccard(target_string.strip(), filtered_output.strip())\n","    return jac, filtered_output\n","\n","\n","def eval_fn(data_loader, model, device):\n","  model.eval()\n","  losses = utils.AverageMeter()\n","  jaccards = utils.AverageMeter()\n","  \n","  with torch.no_grad():\n","    tk0 = tqdm(data_loader, total=len(data_loader))\n","    for bi, d in enumerate(tk0):\n","      ids = d[\"ids\"]\n","      mask = d[\"mask\"]\n","      token_type_ids = d[\"token_type_ids\"]\n","      start_labels = d[\"start_labels\"]\n","      end_labels = d[\"end_labels\"]\n","      orig_tweet = d[\"orig_tweet\"]\n","      orig_selected = d[\"orig_selected\"]\n","      sentiment = d[\"sentiment\"]\n","      offsets = d[\"offsets\"]\n","\n","      ids = ids.to(device, dtype=torch.long)\n","      token_type_ids = token_type_ids.to(device, dtype=torch.long)\n","      mask = mask.to(device, dtype=torch.long)\n","      start_labels = start_labels.to(device, dtype=torch.float)\n","      end_labels = end_labels.to(device, dtype=torch.float)\n","\n","      outputs_start, outputs_end = model(\n","          ids=ids,\n","          mask=mask,\n","          token_type_ids=token_type_ids\n","      )\n","      \n","      loss = loss_fn(outputs_start, outputs_end, start_labels, end_labels)\n","      outputs_start = torch.softmax(outputs_start, dim=1).cpu().detach().numpy()\n","      outputs_end = torch.softmax(outputs_end, dim=1).cpu().detach().numpy()\n","      jaccard_scores = []\n","      for px, tweet in enumerate(orig_tweet):\n","        selected_tweet = orig_selected[px]\n","        tweet_sentiment = sentiment[px]\n","        jaccard_score, _ = calculate_jaccard_score(\n","            original_tweet=tweet,\n","            target_string=selected_tweet,\n","            sentiment_val=tweet_sentiment,\n","            idx_start=np.argmax(outputs_start[px, :]),\n","            idx_end=np.argmax(outputs_end[px, :]),\n","            offsets=offsets[px]\n","        )\n","        jaccard_scores.append(jaccard_score)\n","\n","      jaccards.update(np.mean(jaccard_scores), ids.size(0))\n","      losses.update(loss.item(), ids.size(0))\n","      tk0.set_postfix(loss=losses.avg, jaccard=jaccards.avg)\n","  \n","  print(f\"Jaccard = {jaccards.avg}\")\n","  return jaccards.avg"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZeDfkqe8AGdk","colab_type":"text"},"source":["Training"]},{"cell_type":"code","metadata":{"id":"boCDYa0nAHB9","colab_type":"code","colab":{}},"source":["import pandas as pd\n","import torch.nn as nn\n","\n","from transformers import AdamW\n","from transformers import get_linear_schedule_with_warmup\n","\n","def run(fold):\n","    dfx = pd.read_csv(config.TRAIN_FILE)\n","\n","    df_train = dfx[dfx.kfold != fold].reset_index(drop=True)\n","    df_valid = dfx[dfx.kfold == fold].reset_index(drop=True)\n","    \n","    ####################### Create Dataset and make it iterable #######################\n","    train_dataset = TweetDataset(\n","        tweets=df_train.text.values,\n","        sentiments=df_train.sentiment.values,\n","        selected_texts=df_train.selected_text.values\n","    )\n","\n","    train_data_loader = torch.utils.data.DataLoader(\n","        train_dataset,\n","        batch_size=config.TRAIN_BATCH_SIZE,\n","        num_workers=0 # Changed from 4 to 0 - Need further investigation\n","    )\n","\n","    valid_dataset = TweetDataset(\n","        tweets=df_valid.text.values,\n","        sentiments=df_valid.sentiment.values,\n","        selected_texts=df_valid.selected_text.values\n","    )\n","\n","    valid_data_loader = torch.utils.data.DataLoader(\n","        valid_dataset,\n","        batch_size=config.VALID_BATCH_SIZE,\n","        num_workers=0 # Changed from 2 to 0 - Need further investigation\n","    )\n","    ##################################################################################\n","\n","    # Load BertConfig\n","    device = torch.device(\"cuda\")\n","    model_config = transformers.BertConfig.from_pretrained(config.MODEL_CONFIG)\n","    model_config.output_hidden_states = True\n","    model = TweetModel(conf=model_config)\n","    model.to(device)\n","\n","    num_train_steps = int(len(df_train) / config.TRAIN_BATCH_SIZE * config.EPOCHS)\n","    param_optimizer = list(model.named_parameters())\n","    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n","    optimizer_parameters = [\n","        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n","        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n","    ]\n","    optimizer = AdamW(optimizer_parameters, lr=3e-5)\n","    scheduler = get_linear_schedule_with_warmup(\n","        optimizer, \n","        num_warmup_steps=0, \n","        num_training_steps=num_train_steps\n","    )\n","\n","    # Patience can be set to 4\n","    es = utils.EarlyStopping(patience=2, mode=\"max\")\n","    print(f\"Training is Starting for fold={fold}\")\n","    \n","    # I'm training only for 3 epochs even though I specified 5!!!\n","    for epoch in range(config.EPOCHS):\n","      train_fn(train_data_loader, model, optimizer, device, scheduler=scheduler)\n","      jaccard = eval_fn(valid_data_loader, model, device)\n","      print(f\"Jaccard Score = {jaccard}\")\n","      es(jaccard, model, model_path=f\"{config.SAVE_WEIGHTS_PATH}/model_{fold}.bin\")\n","      if es.early_stop:\n","          print(\"Early stopping\")\n","          break"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BopDZT-zAJZF","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":67,"referenced_widgets":["53fb8faa00e3446ca122d11f1baff16e","c51524348f7144ec8ccb8aff2d1b7d8e","17162273cb5840f7b4cd5d01ca9cf77a","b045f3c381dd4b8fb28018e1a4e10c03","4e36c890e8b44ef5b317ded1e7267323","2431c9fcc00a4ad086723020f39b975a","4b73977367a744a1ad7bd8e08fe165cc","6e3292261054423dbd017337294f8652"]},"outputId":"e16d5b46-81a6-4b1f-86fa-8f71d69c1446"},"source":["run(fold=0)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Training is Starting for fold=0\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"53fb8faa00e3446ca122d11f1baff16e","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=688.0), HTML(value='')))"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"hq8IEAGyGboE","colab_type":"code","colab":{}},"source":["run(fold=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r_SvsKKjGb9T","colab_type":"code","colab":{}},"source":["run(fold=2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xjwvfuy3Gc1S","colab_type":"code","colab":{}},"source":["run(fold=3)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"N819bPwIGdoC","colab_type":"code","colab":{}},"source":["run(fold=4)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Eu3WFx12PV-S","colab_type":"text"},"source":["# Do the evaluation on test data\n","\n"]},{"cell_type":"code","metadata":{"id":"6rfaiynvPWlA","colab_type":"code","colab":{}},"source":["df_test = pd.read_csv(config.TEST_FILE)\n","df_test.loc[:, \"selected_text\"] = df_test.text.values"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G8nbR2_Ua3Vx","colab_type":"code","colab":{}},"source":["device = torch.device(\"cuda\")\n","model_config = transformers.RobertaConfig.from_pretrained(config.MODEL_CONFIG)\n","model_config.output_hidden_states = True"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BAmxQOU8a3S5","colab_type":"code","colab":{}},"source":["fold_models = []\n","for i in range(config.N_FOLDS):\n","  model = TweetModel(conf=model_config)\n","  model.to(device)\n","  model.load_state_dict(torch.load(f'{config.SAVE_WEIGHTS_PATH}/model_{i}.bin'))\n","  model.eval()\n","  fold_models.append(model)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jMX42cxka3MV","colab_type":"code","colab":{}},"source":["final_output = []\n","\n","test_dataset = TweetDataset(\n","        tweets=df_test.text.values,\n","        sentiments=df_test.sentiment.values,\n","        selected_texts=df_test.selected_text.values)\n","\n","data_loader = torch.utils.data.DataLoader(\n","    test_dataset,\n","    shuffle=False,\n","    batch_size=config.VALID_BATCH_SIZE,\n","    num_workers=0)\n","\n","with torch.no_grad():\n","  tk0 = tqdm(data_loader, total=len(data_loader))\n","  for bi, d in enumerate(tk0):\n","\n","    ids = d[\"ids\"]\n","    mask = d[\"mask\"]\n","    token_type_ids = d[\"token_type_ids\"]\n","    start_labels = d[\"start_labels\"]\n","    end_labels = d[\"end_labels\"]\n","    orig_tweet = d[\"orig_tweet\"]\n","    orig_selected = d[\"orig_selected\"]\n","    sentiment = d[\"sentiment\"]\n","    offsets = d[\"offsets\"]\n","\n","    ids = ids.to(device, dtype=torch.long)\n","    token_type_ids = token_type_ids.to(device, dtype=torch.long)\n","    mask = mask.to(device, dtype=torch.long)\n","    start_labels = start_labels.to(device, dtype=torch.long)\n","    end_labels = end_labels.to(device, dtype=torch.long)\n","\n","    outputs_start_folds = []\n","    outputs_end_folds = []\n","    for i in range(config.N_FOLDS):\n","      outputs_start, outputs_end = fold_models[i](ids=ids,\n","                                                  mask=mask,\n","                                                  token_type_ids=token_type_ids)\n","      outputs_start_folds.append(outputs_start)\n","      outputs_end_folds.append(outputs_end)\n","\n","    outputs_start = sum(outputs_start_folds) / config.N_FOLDS\n","    outputs_end = sum(outputs_end_folds) / config.N_FOLDS\n","    \n","    outputs_start = torch.softmax(outputs_start, dim=1).cpu().detach().numpy()\n","    outputs_end = torch.softmax(outputs_end, dim=1).cpu().detach().numpy()\n","\n","    for px, tweet in enumerate(orig_tweet):\n","      selected_tweet = orig_selected[px]\n","      tweet_sentiment = sentiment[px]\n","      _, output_sentence = calculate_jaccard_score(\n","          original_tweet=tweet,\n","          target_string=selected_tweet,\n","          sentiment_val=tweet_sentiment,\n","          idx_start=np.argmax(outputs_start[px, :]),\n","          idx_end=np.argmax(outputs_end[px, :]),\n","          offsets=offsets[px]\n","      )\n","      final_output.append(output_sentence)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"usdIONUgfX13","colab_type":"code","colab":{}},"source":["# post-process trick:\n","# Note: This trick comes from: https://www.kaggle.com/c/tweet-sentiment-extraction/discussion/140942\n","# When the LB resets, this trick won't help\n","def post_process(selected):\n","    return \" \".join(set(selected.lower().split()))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rdGrSAfQdtoF","colab_type":"code","colab":{}},"source":["sample = pd.read_csv(config.SAMPLE_SUBMISSION_FILE)\n","sample.loc[:, 'selected_text'] = final_output\n","sample.selected_text = sample.selected_text.map(post_process)\n","sample.to_csv(config.FINAL_SUBMISSION_FILE + '/submission.csv', index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZdmSD2zrdu_j","colab_type":"code","colab":{}},"source":["sample.head()"],"execution_count":null,"outputs":[]}]}